{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7d6d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, \"/home/odyssey/mmk_smoke_detection\")\n",
    "\n",
    "from dataset_preparator.preparator import plot_detections\n",
    "from dataset_preparator.classJson import JsonData\n",
    "from typing import Dict, Optional, List, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362d5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct131401  ct131402  ct131403  ct131404\tct131405  ct131406  ct1314.csv\n",
    "BACKGROUND_LABEL = \"background\"\n",
    "EMISSION_LABEL = \"emission\"\n",
    "FIRE_LABEL = \"fire\"\n",
    "MACHINE_LABEL = \"machine\"\n",
    "\n",
    "LABELS = [BACKGROUND_LABEL, EMISSION_LABEL, FIRE_LABEL, MACHINE_LABEL]\n",
    "PATH_TO_CONF = \"/home/odyssey/mmk_smoke_detection/validation/new_conf\"\n",
    "PATH_TO_VALIDATION_DATA = \"/home/odyssey/mmk_smoke_detection/validation/05.10.21\"\n",
    "\n",
    "CAMERA_KEYS = [\n",
    "    path.split('/')[-1]\n",
    "    for path in glob.glob(os.path.join(PATH_TO_VALIDATION_DATA, '*'))\n",
    "    if os.path.isdir(path)\n",
    "]\n",
    "SQUARE_DIM = 120\n",
    "\n",
    "H, W = 60, 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e69718",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetDirectoryController:\n",
    "    MAIN_DATASET_DIR = \"dataset\"\n",
    "\n",
    "    _path_for_labels: List[str]\n",
    "    _dataset_main_dir: str\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataset_main_dir: Optional[str] = None):\n",
    "        self._path_for_labels = []\n",
    "        self._dataset_main_dir = dataset_main_dir or self.MAIN_DATASET_DIR\n",
    "\n",
    "    @property\n",
    "    def dataset_dir(self) -> str:\n",
    "        return self._dataset_main_dir\n",
    "\n",
    "    def get_directory_for_label_idx(self, label_idx: int) -> str:\n",
    "        return self._path_for_labels[label_idx]\n",
    "\n",
    "    def prepare_directories(self):\n",
    "        if not os.path.exists(self._dataset_main_dir):\n",
    "            os.mkdir(self._dataset_main_dir)\n",
    "        for label in LABELS:\n",
    "            label_path = os.path.join(self._dataset_main_dir, label)\n",
    "            if not os.path.exists(label_path):\n",
    "                os.mkdir(label_path)\n",
    "            self._path_for_labels.append(label_path)\n",
    "\n",
    "    @property\n",
    "    def stats(self) -> Dict[str, int]:\n",
    "        return {\n",
    "            LABELS[label_idx]: len(\n",
    "                glob.glob(\n",
    "                    os.path.join(\n",
    "                        self.get_directory_for_label_idx(label_idx),\n",
    "                        '*'\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "            for label_idx in range(4)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "198a7369",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParsedFilePath:\n",
    "    cfg_key: str\n",
    "    dir_path: str\n",
    "    file_name: str\n",
    "    file_extension: str\n",
    "    full_name: str\n",
    "    label_name: str\n",
    "    \n",
    "    def __init__(self, file_path: str):\n",
    "        self.full_name = file_path\n",
    "        path, self.file_extension = os.path.splitext(file_path)\n",
    "        self.file_name = os.path.basename(path)\n",
    "        self.dir_path = os.path.dirname(path)\n",
    "        self.cfg_key = self.dir_path.split('/')[0]\n",
    "        self.label_name = self.dir_path.split('/')[1]\n",
    "        \n",
    "    def gen_new_path(self, iter_num: int) -> str:\n",
    "        new_file_name = f\"{self.file_name}_{iter_num}{self.file_extension}\"\n",
    "        return os.path.join(self.label_name, new_file_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7152d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigController:    \n",
    "    configs: Dict[str, JsonData]\n",
    "        \n",
    "    @staticmethod\n",
    "    def extract_key_from_conf_filename(\n",
    "        conf_filename: str\n",
    "    ) -> str:\n",
    "        return os.path.basename(conf_filename)[5:13]\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        path_to_configs: str,\n",
    "    ):\n",
    "        self.configs = {}\n",
    "        conf_files = glob.glob(os.path.join(path_to_configs, '*'))\n",
    "        print(conf_files)\n",
    "        for conf_file in conf_files:\n",
    "            conf_key = self.extract_key_from_conf_filename(conf_file)\n",
    "            conf_data = JsonData(conf_file)\n",
    "            self.configs[conf_key] = conf_data\n",
    "            \n",
    "    def __getitem__(\n",
    "        self,\n",
    "        key: str\n",
    "    ) -> JsonData:\n",
    "        return self.configs[key]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eb7775e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05.10.21\t\t\t   __init__.py\t     trdt.py\r\n",
      "05.10.21_annotated_ct1314.7z\t   new_conf\t     validate_detector.py\r\n",
      "box_test_draw.jpg\t\t   not_expanded_val  val_visual.ipynb\r\n",
      "expanded_val\t\t\t   test_draw.jpg     vanilla_val_res.ipynb\r\n",
      "generate_validation_dataset.ipynb  test_pers.jpg\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651951f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationDatasetGenerator:\n",
    "    ID = 'paths'\n",
    "\n",
    "    _src_dataset: str\n",
    "    _dst_dataset: str\n",
    "    _table: str\n",
    "    _need_expand: bool\n",
    "    _configs: ConfigController\n",
    "        \n",
    "    _dst_dir_con: DatasetDirectoryController\n",
    "        \n",
    "    _skipped_count: int\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        config_controller: ConfigController, \n",
    "        src_dataset: str,\n",
    "        dst_dataset: str,\n",
    "        path_to_csv: str,\n",
    "        need_expand: bool = True\n",
    "    ):\n",
    "        self._configs = config_controller\n",
    "        self._src_dataset = src_dataset\n",
    "        self._dst_dataset = dst_dataset\n",
    "        self._table = pd.read_csv(path_to_csv)\n",
    "        self._need_expand = need_expand\n",
    "        self._skipped_count = 0\n",
    "        self._dst_dir_con = DatasetDirectoryController(dst_dataset)\n",
    "        \n",
    "        if os.path.exists(dst_dataset):\n",
    "            shutil.rmtree(dst_dataset)\n",
    "        self._dst_dir_con.prepare_directories()\n",
    "#             raise Exception(f\"Dst dir {dst_dataset} exists, remove it or change it!\")\n",
    "#         shutil.copytree(src_dataset, dst_dataset, ignore=shutil.ignore_patterns('*.jpg', '*.png', '*.csv'))\n",
    "    \n",
    "    @staticmethod\n",
    "    def _increase_square_params(\n",
    "            dim: int,\n",
    "            c_less: int,\n",
    "            c_grt: int,\n",
    "            max_value: int\n",
    "    ) -> Tuple[int, int]:\n",
    "        add_diff = (dim - (c_grt - c_less)) // 2\n",
    "        less_diff = c_less - add_diff\n",
    "        grt_diff = max_value - (c_grt + add_diff)\n",
    "\n",
    "        if less_diff < 0:\n",
    "            # эту разницу надо добавить к inc_c_grt\n",
    "            inc_c_less = 0\n",
    "            inc_c_grt = c_grt + add_diff + np.abs(less_diff)\n",
    "        elif grt_diff < 0:\n",
    "            # эту разницу надо добавить к inc_c_less\n",
    "            inc_c_grt = max_value\n",
    "            inc_c_less = c_less - add_diff - np.abs(grt_diff)\n",
    "        else:\n",
    "            inc_c_less = c_less - add_diff\n",
    "            inc_c_grt = c_grt + add_diff\n",
    "\n",
    "        if inc_c_grt - inc_c_less < SQUARE_DIM:\n",
    "            if inc_c_grt + 1 >= max_value:\n",
    "                inc_c_less -= 1\n",
    "            else:\n",
    "                inc_c_grt += 1\n",
    "\n",
    "        return inc_c_less, inc_c_grt\n",
    "\n",
    "    def _expand_box(self,\n",
    "                    max_height: int,\n",
    "                    max_width: int,\n",
    "                    box: np.ndarray) -> List[int]:\n",
    "        if self._need_expand:\n",
    "            new_min_y, new_max_y = ValidationDatasetGenerator._increase_square_params(\n",
    "                H,\n",
    "                box[1],\n",
    "                box[3],\n",
    "                max_value=max_height\n",
    "            )\n",
    "            new_min_x, new_max_x = ValidationDatasetGenerator._increase_square_params(W,\n",
    "                                                                                      box[0],\n",
    "                                                                                      box[2],\n",
    "                                                                                      max_value=max_width)\n",
    "        else:\n",
    "            new_min_x, new_min_y = box[0], box[1]\n",
    "            new_max_x, new_max_y = box[2], box[3]\n",
    "        if new_min_x < 0:\n",
    "            new_min_x = 0\n",
    "        if new_min_y < 0:\n",
    "            new_min_y = 0\n",
    "        if new_max_x > max_width:\n",
    "            new_max_x = max_width\n",
    "        if new_max_y > max_height:\n",
    "            new_max_y = max_height\n",
    "        return [new_min_x, new_min_y, new_max_x, new_max_y]\n",
    "\n",
    "    def _crop_by_place_and_save(\n",
    "        self,\n",
    "        img: np.ndarray,\n",
    "        box: List[int],\n",
    "        num_iter: int,\n",
    "        parsed_file: ParsedFilePath\n",
    "    ):\n",
    "        cropped_image = img[box[1]: box[3], box[0]: box[2]]\n",
    "        croopped_img_name = parsed_file.gen_new_path(num_iter)\n",
    "        path_to_save = os.path.join(\n",
    "            self._dst_dataset,\n",
    "            croopped_img_name\n",
    "        )\n",
    "        try:\n",
    "            Image.fromarray(cropped_image).save(path_to_save)\n",
    "        except Exception:\n",
    "            print(img.shape, box)\n",
    "        \n",
    "    def _apply_perspective_to_img(\n",
    "        self,\n",
    "        file_path: str,\n",
    "        pers_matrix: Dict\n",
    "    ) -> np.ndarray:\n",
    "        frame = Image.open(\n",
    "            os.path.join(\n",
    "                self._src_dataset,\n",
    "                file_path\n",
    "            )\n",
    "        )\n",
    "        frame = cv2.warpPerspective(\n",
    "            np.array(frame),\n",
    "            np.array(pers_matrix['matrix']),\n",
    "            (pers_matrix['maxWidth'], pers_matrix['maxHeight']),\n",
    "            flags=cv2.INTER_LINEAR\n",
    "        )\n",
    "        return frame\n",
    "        \n",
    "    def _apply_perspective_to_box(\n",
    "        self,\n",
    "        boxes: np.ndarray,\n",
    "        pers_matrix: Dict\n",
    "    ) -> np.ndarray:\n",
    "        reshape_boxes = np.float32(boxes.reshape(-1)).reshape(-1, 1, 2)\n",
    "        mat = np.array(pers_matrix['matrix'])\n",
    "        return cv2.perspectiveTransform(reshape_boxes, mat).reshape(4).astype(int).reshape(-1, 4)\n",
    "        \n",
    "    def _handle_image(\n",
    "        self,\n",
    "        file: str\n",
    "    ):\n",
    "        parsed_file = ParsedFilePath(file)\n",
    "        file_part = self._table[self._table[self.ID] == file][['x_min', 'y_min', 'x_max', 'y_max', 'class']].to_numpy()\n",
    "        file_boxes = file_part[:, :4]\n",
    "        file_classes = file_part[:, 4]\n",
    "        \n",
    "        pers_matrix = self._configs[parsed_file.cfg_key].return_matrix()\n",
    "        pers_img = self._apply_perspective_to_img(file_path=file, pers_matrix=pers_matrix)\n",
    "        pers_boxes = self._apply_perspective_to_box(file_boxes, pers_matrix)\n",
    "        \n",
    "        max_height, max_width = pers_img.shape[0], pers_img.shape[1]\n",
    "        pers_boxes = [\n",
    "            self._expand_box(\n",
    "                max_height=max_height,\n",
    "                max_width=max_width,\n",
    "                box=box\n",
    "            )\n",
    "            for box in pers_boxes\n",
    "        ]\n",
    "        for num_iter, box in enumerate(pers_boxes):\n",
    "            self._crop_by_place_and_save(\n",
    "                img=pers_img,\n",
    "                box=box,\n",
    "                num_iter=num_iter,\n",
    "                parsed_file=parsed_file\n",
    "            )\n",
    "                     \n",
    "        \n",
    "    def handle_images(\n",
    "        self\n",
    "    ):\n",
    "        files = self._table[self.ID].unique()\n",
    "        for file in tqdm.tqdm(files):\n",
    "            if not os.path.exists(os.path.join(self._src_dataset, file)):\n",
    "                self._skipped_count += 1\n",
    "                continue\n",
    "            self._handle_image(file)\n",
    "        print(\"SKIPPED FILES COUNT:\", self._skipped_count)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4cbce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_conf/conf_ct131406.json', 'new_conf/conf_ct131405.json', 'new_conf/conf_ct131402.json', 'new_conf/conf_ct131401.json', 'new_conf/conf_ct131403.json', 'new_conf/conf_ct131404.json']\n"
     ]
    }
   ],
   "source": [
    "cfg_controller = ConfigController(\n",
    "    path_to_configs='new_conf'\n",
    ")\n",
    "\n",
    "val_dataset_gen = ValidationDatasetGenerator(\n",
    "    config_controller=cfg_controller,\n",
    "    src_dataset='05.10.21',\n",
    "    dst_dataset='expanded_val',\n",
    "    path_to_csv='05.10.21/ct1314.csv',\n",
    "    need_expand=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09af7fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 661/661 [00:17<00:00, 38.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED FILES COUNT: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_dataset_gen.handle_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f0280323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new_conf/conf_ct131406.json', 'new_conf/conf_ct131405.json', 'new_conf/conf_ct131402.json', 'new_conf/conf_ct131401.json', 'new_conf/conf_ct131403.json', 'new_conf/conf_ct131404.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 661/661 [00:18<00:00, 36.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKIPPED FILES COUNT: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cfg_controller = ConfigController(\n",
    "    path_to_configs='new_conf'\n",
    ")\n",
    "\n",
    "vanilla_val_dataset_gen = ValidationDatasetGenerator(\n",
    "    config_controller=cfg_controller,\n",
    "    src_dataset='05.10.21',\n",
    "    dst_dataset='not_expanded_val',\n",
    "    path_to_csv='05.10.21/ct1314.csv',\n",
    "    need_expand=False\n",
    ")\n",
    "vanilla_val_dataset_gen.handle_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fa7739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "background  fire\r\n"
     ]
    }
   ],
   "source": [
    "! rm -r line_fire_binary_val\n",
    "! cp -r line_three_val line_fire_binary_val\n",
    "! rm -r line_fire_binary_val/emission\n",
    "! ls line_fire_binary_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e115583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
